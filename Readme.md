# Sparse Probing for Mechanistic Interpretability

This repository contains a Python framework for exploring the internal representations of large language models (LLMs) using **sparse probing**. The goal is to identify the specific neurons or features within a model's activations that are responsible for performing a particular task.

The implementation is primarily inspired by the work of Gurnee et al. in **"Finding Neurons in a Haystack"**, which demonstrates how sparse, linear probes can uncover localized and interpretable features in LLMs. The L0-regularized probes are based on the method for learning truly sparse networks proposed by Louizos et al. in **"Learning Sparse Neural Networks through L0 Regularization"**.

## Project Overview

At its core, this project seeks to answer the question: "Where does a model like Google's Gemma store specific pieces of information?"

Instead of treating the model as a complete black box, we can "probe" its internal state. We do this by extracting the activations from various layers of the model as it processes a task-specific dataset (in this case, the Indirect Object Identification or IOI task).

Then, we train very simple, linear models (the "probes") to predict the correct answer using only these activations. The key is that we force these probes to be **sparse**â€”meaning they are heavily penalized for using more than a handful of features from the thousands available. If a sparse probe can still perform the task well, it implies that the information required is concentrated in just a few "specialist" neurons.

We used linear probes because of the **Linear Representation Hypothesis** (basically features are represented linearly like "directions" in the activation space)

### Example Results

The following plot was generated by running the experiment on the `google/gemma-3-1b-it` model for the IOI task.

![Sparse Probing Results](https://user-images.githubusercontent.com/copilot/94575953-33333333-3333-3333-3333-333333333333)

**Analysis of the Plots:**
1.  **Accuracy vs. Layer (Top-Left):** This shows that `dense` probes (which use all features) achieve the highest accuracy, as expected. It also highlights that model performance on the IOI task varies significantly by layer, with later layers (e.g., 19-23) generally performing better.
2.  **Accuracy vs. Sparsity Level (Top-Right):** This plot confirms that as we allow the `k-sparse` probes to use more features (increasing `k`), their accuracy generally improves across all layers.
3.  **Probe Type Comparison (Bottom-Left):** This summarizes the average performance. Dense probes are best, the sparse probes (`k-sparse` and `l0_reg`) barely performing better than random chance (`k-sparse` is generally better than `l0_reg`).
4.  **Best Performance by Layer (Bottom-Right):** This plot shows the best accuracy achieved on each layer. It confirms that dense probes find the optimal solution but also helps pinpoint which layers are most crucial for the task.

## How It's Implemented

The framework is designed to be modular and easy to understand.

1.  **Activation Extraction (`interpretability/framework.py`):**
    *   A `MechanisticInterpretabilityFramework` class loads the specified Hugging Face model (e.g., `gemma-3-1b-it`).
    *   It uses PyTorch's `register_forward_hook` to attach "listeners" to the specified layers of the model.
    *   When text is passed through the model, these hooks capture the activation tensors from each hooked layer without permanently modifying the model.

2.  **Data Handling (`interpretability/data.py`):**
    *   The `IOIDatasetProcessor` class handles downloading and parsing the `fahamu/ioi` dataset.
    *   It prepares the prompts (text) and the corresponding correct answers (labels).

3.  **Sparse Probing (`interpretability/probing.py`):**
    *   This is the core of the project. The `SparseProbeFramework` class orchestrates the experiment.
    *   For each layer's activations, it trains and evaluates three types of probes:
        *   **`Dense Probe`**: A standard `nn.Linear` layer. This is our baseline and uses all features.
        *   **`k-Sparse Probe`**: A custom probe that identifies the `k` features with the highest importance scores and uses only them. This directly enforces a hard sparsity constraint.
        *   **`L0-Regularized Probe`**: A probe that uses L0 regularization. This technique adds a penalty to the loss function that encourages weights to become exactly zero, effectively turning features off. It *learns* which features to discard to achieve sparsity.
    *   The framework iterates through all specified layers and `k` values, trains the probes, evaluates their accuracy, and stores all results.

4.  **Execution and Configuration:**
    *   `run_experiment.py`: The main script that ties everything together. It initializes the frameworks, gets the data, extracts activations, runs the probing experiment, and generates the final plots.
    *   `interpretability/config.py`: A central file to easily configure the model name, target layers, and hyperparameters for the probes (like learning rate, epochs, and `k` values).

## How to Run the Experiment

1.  **Clone the repository:**
    ````bash
    git clone https://github.com/Abdellahbado/Sparse-Probing-Gemma-3
    cd Sparse-Probing-Gemma-3
    ````

2.  **Install dependencies:**
    ````bash
    pip install -r requirements.txt
    ````

3.  **Set up your environment:**
    *   Create a `.env` file in the root directory.
    *   Add your Hugging Face token to this file to access gated models like Gemma:
        ```
        HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxx
        ```

4.  **Configure the experiment (optional):**
    *   Modify `interpretability/config.py` to change the `MODEL_NAME`, `TARGET_LAYERS`, or `PROBE_CONFIG` hyperparameters.

5.  **Run the experiment:**
    ````bash
    python run_experiment.py
    ````
    Results, including the plots and a `probing_results.json` file, will be saved in the `results/` directory.

## References

> Gurnee, W., Nanda, N., Pauly, M., Harvey, K., Troitskii, D., & Bertsimas, D. (2023). *Finding Neurons in a Haystack: Case Studies with Sparse Probing*. arXiv preprint arXiv:2305.01610.

> Louizos, C., Welling, M., & Kingma, D. P. (2017). *Learning Sparse Neural Networks through L0 Regularization*. arXiv preprint arXiv:1712.01312.